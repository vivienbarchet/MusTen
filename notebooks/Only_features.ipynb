{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import librosa.display\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import dissonant\n",
    "import os\n",
    "import scipy\n",
    "import spleeter\n",
    "import numpy as np\n",
    "import mir_eval\n",
    "import crepe\n",
    "import IPython.display as ipd\n",
    "from predict_on_audio1 import compute_hcqt,compute_output\n",
    "import os\n",
    "import soundfile\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "\n",
    "from scipy import signal\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31269e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempo_extraction(y, sr, start_bpm = 80):\n",
    "    print(\"Estimating tempo...\")\n",
    "    #Constant Q-spectrogram\n",
    "    C = np.abs(librosa.cqt(y=y, sr=sr))\n",
    "    o_env3 = librosa.onset.onset_strength(sr=sr,S = librosa.amplitude_to_db(C, ref=np.max))\n",
    "    \n",
    "    times3 = librosa.frames_to_time(np.arange(len(o_env3)), sr=sr)\n",
    "\n",
    "\n",
    "\n",
    "    # Obtain dynamic tempo\n",
    "    dtempo3 = librosa.beat.tempo(y = y, sr = sr, onset_envelope = o_env3, aggregate = None, start_bpm = start_bpm)\n",
    "    #t = librosa.frames_to_time(np.arange(len(dtempo3)))\n",
    "    \n",
    "\n",
    "    \n",
    "    return dtempo3, times3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_individual_features(df_plot, feature):\n",
    "    plt.figure(figsize=(20,4))\n",
    "\n",
    "    if feature == \"tempo\":\n",
    "        plt.plot(df_plot['time'], df_plot['tempo_z'], color='r', linewidth=1.5, label='Tempo estimate')\n",
    "        plt.title('Dynamic tempo estimation')\n",
    "        #plt.legend(frameon=True, framealpha=0.75)\n",
    "\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('BPM')\n",
    "    \n",
    "    elif feature == \"onset_frequency\":\n",
    "        \n",
    "        #Plot the smoothed curve\n",
    "        plt.plot(df_plot['time'], df_plot['onset_freq_z'])\n",
    "\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Onset Density\")\n",
    "        plt.title(\"Onset Density\")\n",
    "    \n",
    "    elif feature == \"loudness\":\n",
    "        \n",
    "        plt.plot(df_plot['time'], df_plot['loudness_z'])\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Loudness (dB)')\n",
    "        plt.title('Loudness')\n",
    "    \n",
    "    elif feature == \"dissonance\":\n",
    "        #Plot the smoothed curve\n",
    "        plt.plot(df_plot['time'], df_plot['dissonance_z'])\n",
    "\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Dissonance (smoothed)\")\n",
    "        plt.title(\"Smoothed dissonance curve\")\n",
    "        \n",
    "    elif feature == \"pitch\":\n",
    "        \n",
    "        pitch_df = df_plot.loc[:, np.logical_or(df_plot.columns.str.startswith('line'), df_plot.columns == 'pitch')]\n",
    "        pitch_df['time'] = df_plot['time']\n",
    "                \n",
    "        for col in pitch_df.columns[:-1]:\n",
    "            plt.plot(pitch_df.time, pitch_df[col]) \n",
    "        \n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Pitch height\")\n",
    "        plt.title(\"Pitch height\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempo_evaluation(y,sr, start_bpm = 80):\n",
    "    times, beats = librosa.beat.beat_track(y=y, sr=sr, start_bpm=start_bpm)\n",
    "    y_beats = librosa.clicks(frames=beats, sr=sr, length=len(y))\n",
    "    aud = y + y_beats\n",
    "    \n",
    "    display(ipd.Audio(y+ y_beats, rate = sr))   \n",
    "    return aud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e04717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onset_freq_detection(y,sr):\n",
    "    o_env = librosa.onset.onset_strength(y = y, sr=sr)\n",
    "\n",
    "    onset_frames = librosa.onset.onset_detect(onset_envelope=o_env, sr=sr)\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "    #Onset frequency = maximum event duration - current event duration\n",
    "    diffs = np.diff(onset_times)\n",
    "\n",
    "    max_diff = np.max(diffs)\n",
    "    max_index = np.where(diffs == max_diff)\n",
    "\n",
    "    # diff between max event and all events\n",
    "    onset_freq2 = max_diff - diffs\n",
    "\n",
    "    # remove first instance for plotting\n",
    "    onset_times_rm2 = onset_times[1:,]\n",
    "\n",
    "\n",
    "    #Smooth the signal\n",
    "    wind = int((len(onset_freq2)/max(onset_times_rm2))*2)\n",
    "\n",
    "    onset_density_smooth = uniform_filter1d(onset_freq2, size =wind) \n",
    "\n",
    "\n",
    "    return o_env, onset_frames, onset_density_smooth, onset_times_rm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onset_frequency(y,sr):\n",
    "    print(\"Estimating Onset frequency...\")\n",
    "    o_env = librosa.onset.onset_strength(y = y, sr=sr)\n",
    "\n",
    "    onset_frames = librosa.onset.onset_detect(onset_envelope=o_env, sr=sr)\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "\n",
    "    \n",
    "    ##Buildung in the adjustment for pieces with strong character changes leading to no onset detection by librosa\n",
    "    onset_times_with_end = np.append(onset_frames, len(y)/512)\n",
    "    onset_times_start_end = np.insert(onset_times_with_end, 0, 0, axis=0)\n",
    "    \n",
    "    onset_diffs_2 = np.diff(onset_times_start_end)\n",
    "    \n",
    "    if max(onset_diffs_2)>(5*sr)/512:\n",
    "        print(\"The audio includes large parts > 5s in which no onsets are detected.\")\n",
    "\n",
    "        if (max(onset_diffs_2) == onset_diffs_2[-1]):\n",
    "            print(\"The part without onset detections is in the end. Will now split the audio for onset detection.\")\n",
    "\n",
    "            sep = int(max(onset_frames)*512)\n",
    "            y_1 = y[:sep+1]\n",
    "            y_2 = y[sep+1:]\n",
    "\n",
    "            o_env_1, onset_frames_1, density1, times_1 = onset_freq_detection(y_1, sr)\n",
    "            o_env_2, onset_frames_2, density2, times_2 = onset_freq_detection(y_2, sr)\n",
    "\n",
    "            onset_frames_1 = onset_frames_1[1:]\n",
    "            onset_frames_2 = onset_frames_2[1:]\n",
    "            onset_frames_2 = onset_frames_2 + max(onset_frames_1)\n",
    "            onset_frames = np.append(onset_frames_1, onset_frames_2)\n",
    "            \n",
    "            onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "\n",
    "            onset_envelope = np.append(o_env_1, o_env_2)\n",
    "            onset_density_smooth = np.append(density1, density2)\n",
    "            \n",
    "        elif (max(onset_diffs_2) == onset_diffs_2[0]):\n",
    "            \n",
    "            print(\"The part without onset detections is in the beginning. Will now split the audio for onset detection.\")\n",
    "            \n",
    "            sep = int(min(onset_frames)*512)\n",
    "            y_1 = y[:sep+1]\n",
    "            y_2 = y[sep+1:]\n",
    "\n",
    "            o_env_1, onset_frames_1, density1, times_1 = onset_freq_detection(y_1, sr)\n",
    "            o_env_2, onset_frames_2, density2, times_2 = onset_freq_detection(y_2, sr)\n",
    "\n",
    "            onset_frames_1 = onset_frames_1[1:]\n",
    "            onset_frames_2 = onset_frames_2[1:]\n",
    "            onset_frames_2 = onset_frames_2 + max(onset_frames_1)\n",
    "            onset_frames = np.append(onset_frames_1, onset_frames_2)\n",
    "            \n",
    "            onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "\n",
    "            onset_envelope = np.append(o_env_1, o_env_2)\n",
    "            onset_density_smooth = np.append(density1, density2)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print(\"The part without onset detections is in the middle. Will calculate onset frequency normally but might not be accurate.\")\n",
    "            onset_envelope, onset_frames, onset_density_smooth, onset_times = onset_freq_detection(y,sr)\n",
    "\n",
    "    else:\n",
    "        onset_envelope, onset_frames, onset_density_smooth, onset_times = onset_freq_detection(y,sr)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    return onset_envelope, onset_density_smooth, onset_times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6756b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onset_evaluation(onset_envelope,y,sr):\n",
    "    onset_times = librosa.onset.onset_detect(onset_envelope=onset_envelope, sr=sr, units = \"time\")\n",
    "    y_onsets = librosa.clicks(times=onset_times, sr=sr, length=len(y))\n",
    "    aud = y + y_onsets\n",
    "    \n",
    "    display(ipd.Audio(y+ y_onsets, rate = sr))   \n",
    "    return aud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b39b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loudness\n",
    "def extract_loudness(y, sr):\n",
    "    print(\"Estimating loudness...\")\n",
    "\n",
    "    S, phase = librosa.magphase(librosa.stft(y))\n",
    "    rms = librosa.feature.rms(S=S)\n",
    "\n",
    "    loudness = librosa.amplitude_to_db(rms)\n",
    "    t = librosa.frames_to_time(np.arange(len(loudness[0])), sr=sr)\n",
    "\n",
    "    return loudness,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_extraction(audio_fpath,sr, category = [\"voice\", \"polyphonic\", \"solo\"], \n",
    "                     melody_lower = 'C2', melody_higher = 'C6'):\n",
    "    print(\"Estimating Pitch...\")\n",
    "    if category == \"voice\":\n",
    "        !spleeter separate -p spleeter:2stems -o output $audio_fpath\n",
    "        \n",
    "        #Loading both audio files (voice & background)\n",
    "        file_name = \"vocals\"\n",
    "        path = \"./output/schubert/\"\n",
    "\n",
    "        audio_fpath = path + file_name + '.wav'\n",
    "\n",
    "        # load vocal file\n",
    "        y_voice, sr = librosa.load(audio_fpath, sr = sr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ##First, we extract the pitch from the melody using pyin\n",
    "        f0, voiced_flag, voiced_probs = librosa.pyin(y_voice,\n",
    "                                             fmin=librosa.note_to_hz(melody_lower),\n",
    "                                             fmax=librosa.note_to_hz(melody_higher), sr = sr, \n",
    "                                                    fill_na = np.nan)\n",
    "\n",
    "        times = librosa.frames_to_time(np.arange(len(f0)), sr = sr)\n",
    "        \n",
    "        tup = zip(times, f0)\n",
    "        df_pitch = pd.DataFrame(tup, columns = ['time', 'pitch'])\n",
    "\n",
    "        #Fill smaller gaps with previous pitches\n",
    "        df_pitch = df_pitch.fillna(limit=100, method='ffill')\n",
    "        #And larger gaps with zeros (breaks in the signal)\n",
    "        df_pitch = df_pitch.fillna(0)\n",
    "\n",
    "        \n",
    "        \n",
    "        ##Now, we extract the pitches from the background\n",
    "        audio_fpath = \"/Users/alice-vivien.barchet/claire_tension/output/schubert/accompaniment.wav\"\n",
    "        task = \"multif0\"\n",
    "        output_format = \"multif0\"\n",
    "        threshold = 0.6\n",
    "        use_neg = False\n",
    "        sr = sr\n",
    "\n",
    "        # this is slow for long audio files\n",
    "        print(\"Computing HCQT...\")\n",
    "        hcqt, freq_grid, time_grid = compute_hcqt(audio_fpath)\n",
    "\n",
    "        times, freqs = compute_output(\n",
    "            hcqt, time_grid, freq_grid, task, output_format,\n",
    "            threshold, use_neg)\n",
    "        \n",
    "        freqs_df = pd.DataFrame(freqs)\n",
    "        \n",
    "        colnames_new = []\n",
    "        ncol = freqs_df.shape[1]\n",
    "        \n",
    "        for col in range(1,ncol+1):\n",
    "            colname = \"l\"+str(col)\n",
    "            colnames_new.append(colname)\n",
    "            \n",
    "        freqs_df.columns = colnames_new\n",
    "        \n",
    "        times_df = pd.DataFrame(times, columns = [\"time\"])\n",
    "        pitches_df = pd.merge(times_df, freqs_df, left_index = True, right_index = True)\n",
    "        \n",
    "        zero_df = pd.DataFrame(np.arange(len(y_voice)), columns = [\"time\"])\n",
    "        zero_df[\"time\"] = zero_df['time']/sr\n",
    "        \n",
    "        pitch_background = pd.merge(zero_df, pitches_df, on = \"time\", how = \"left\")\n",
    "        pitch_melody_background = pd.merge(pitch_background, df_pitch, on = \"time\", how = \"left\")\n",
    "\n",
    "        \n",
    "        pitch_df_new = pitch_melody_background.fillna(method = \"ffill\")\n",
    "\n",
    "    \n",
    "        return pitch_df_new\n",
    "    \n",
    "    elif category == \"polyphonic\":\n",
    "        y, sr = librosa.load(audio_fpath, sr = sr)\n",
    "\n",
    "        print('Estimating polyphone pitch using Deep Salience')\n",
    "        task = \"multif0\"\n",
    "        output_format = \"multif0\"\n",
    "        threshold = 0.3\n",
    "        use_neg = False\n",
    "        sr = 44100\n",
    "\n",
    "        # this is slow for long audio files\n",
    "        print(\"Computing HCQT...\")\n",
    "        hcqt, freq_grid, time_grid = compute_hcqt(audio_fpath)\n",
    "\n",
    "        times,freqs = compute_output(\n",
    "            hcqt, time_grid, freq_grid, task, output_format,\n",
    "            threshold, use_neg)\n",
    "        \n",
    "        freqs_df = pd.DataFrame(freqs)\n",
    "        \n",
    "        colnames_new = []\n",
    "        ncol = freqs_df.shape[1]\n",
    "        \n",
    "        for col in range(1,ncol+1):\n",
    "            colname = \"line\"+str(col)\n",
    "            colnames_new.append(colname)\n",
    "            \n",
    "        freqs_df.columns = colnames_new\n",
    "        \n",
    "        times_df = pd.DataFrame(times, columns = [\"time\"])\n",
    "        pitches_df = pd.merge(times_df, freqs_df, left_index = True, right_index = True)\n",
    "        \n",
    "        #merge with y times \n",
    "        zero_df = pd.DataFrame(np.arange(len(y)), columns = [\"time\"])\n",
    "        zero_df[\"time\"] = zero_df['time']/sr\n",
    "        \n",
    "        pitches_df_resampled = pd.merge(zero_df, pitches_df, on = \"time\", how = \"left\")\n",
    "        #Fill smaller gaps with previous pitches\n",
    "        pitches_df = pitches_df_resampled.fillna(method='ffill', limit = 5000)\n",
    "        #And larger gaps with zeros (breaks in the signal)\n",
    "        pitches_df = pitches_df.fillna(0)\n",
    "        \n",
    "       \n",
    "        return pitches_df\n",
    "    \n",
    "    elif category == \"solo\":\n",
    "        print('Estimating monophone pitch using pyin')\n",
    "\n",
    "        # load audio file\n",
    "        y, sr = librosa.load(audio_fpath, sr = sr)\n",
    "\n",
    "\n",
    "        ##First, we extract the pitch from the melody using pyin\n",
    "        f0, voiced_flag, voiced_probs = librosa.pyin(y,\n",
    "                                             fmin=librosa.note_to_hz(melody_lower),\n",
    "                                             fmax=librosa.note_to_hz(melody_higher), sr = sr)\n",
    "\n",
    "        times = librosa.frames_to_time(np.arange(len(f0)), sr = sr)\n",
    "        \n",
    "        tup = zip(times, f0)\n",
    "        df_pitch = pd.DataFrame(tup, columns = ['time', 'pitch'])\n",
    "\n",
    "        #Fill smaller gaps with previous pitches\n",
    "        df_pitch = df_pitch.fillna(limit=100, method='ffill')\n",
    "        #And larger gaps with zeros (breaks in the signal)\n",
    "        df_pitch = df_pitch.fillna(0)\n",
    "        \n",
    "   \n",
    "        return df_pitch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pitch(pitches, times, sr):\n",
    "    tup = zip(pitches, times)\n",
    "    df_pitch = pd.DataFrame(tup, columns = [\"pitches\", \"times\"])\n",
    "    df_pitch = df_pitch.dropna()\n",
    "    #Sonify the melody pitches to check \n",
    "    son = mir_eval.sonify.pitch_contour(df_pitch.times,df_pitch.pitches, sr)\n",
    "    display(ipd.Audio(son, rate = sr))\n",
    "    return son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d4003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_polyphonic_pitch(pitch_df, sr):\n",
    "    #Sonify the melody pitches to check \n",
    "    sonified_pitches = []\n",
    "    for col in pitch_df.columns:\n",
    "        if col != \"time\":\n",
    "            new_df = pitch_df.fillna(0)\n",
    "            son = mir_eval.sonify.pitch_contour(new_df.time,new_df[col], sr)\n",
    "            sonified_pitches.append(son)\n",
    "    \n",
    "    \n",
    "    for i,s in enumerate(sonified_pitches):\n",
    "        if i == 0:\n",
    "            all_aud = s\n",
    "        else:\n",
    "            all_aud = all_aud + s\n",
    "            \n",
    "    display(ipd.Audio(all_aud, rate = sr))\n",
    "    return all_aud, sonified_pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissonance_extraction(pitch_df, sr):\n",
    "# Downsample so we will get one estimate for dissonance each 10 ms (for computation speed)\n",
    "    print(\"Estimating dissonance...\")\n",
    "    #Resampling to new sampling rate of 100 Hz \n",
    "    pitches_downsampled = pitch_df.iloc[::int(sr/100), :]\n",
    "    pitches_downsampled = pitches_downsampled.reset_index()\n",
    "\n",
    "    dissonance_model_str = 'sethares1993'\n",
    "    n_consonant_partials = 5\n",
    "    times = pitches_downsampled['time']\n",
    "    dissonances = []\n",
    "\n",
    "    for i,time in pitches_downsampled.iterrows():\n",
    "\n",
    "        # Convert peak locations to peak frequencies in Hertz\n",
    "        peak_freqs = np.array(pitches_downsampled.iloc[i,2:])\n",
    "\n",
    "        # If there are less than 2 peaks, append a NaN (\"not a number\") value\n",
    "        if np.count_nonzero(~np.isnan(peak_freqs))<2:\n",
    "            max_pairwise_dissonance = np.nan\n",
    "        else:\n",
    "        # Otherwise, append maximum amount of dissonance\n",
    "        # between lowest frequency and all other frequencies\n",
    "            for freq in peak_freqs:\n",
    "                if ~np.isnan(freq):\n",
    "                    pairwise_dissonances = []\n",
    "                    h_freqs, h_amps = dissonant.harmonic_tone([peak_freqs[0], freq], n_partials=n_consonant_partials)\n",
    "                    d = dissonant.dissonance(h_freqs, h_amps, model=dissonance_model_str)\n",
    "                    pairwise_dissonances.append(d)\n",
    "                    max_pairwise_dissonance = np.max(pairwise_dissonances)\n",
    "\n",
    "        dissonances.append(max_pairwise_dissonance)\n",
    "    \n",
    "    #Smooth the dissonance\n",
    "    #Calculate the window (2 seconds)\n",
    "    wind = (len(dissonances)/max(times))*2\n",
    "\n",
    "    #Rounding to the next odd integer (because we need an odd integer for the smoothing function)\n",
    "    window_round = int(np.ceil(wind / 2.) * 2 - 1)\n",
    "\n",
    "    dissonance_smooth = savgol_filter(dissonances, window_round, 3) \n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    return dissonance_smooth, times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfef877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissonance_extraction_1(y,sr):\n",
    "    \n",
    "    # Define spectrogram parameters\n",
    "    hop_length = 512\n",
    "    win_length = 2048\n",
    "    n_fft = 2048\n",
    "\n",
    "    # Define perceptual threshold for multipitch estimation\n",
    "    loudness_threshold = 10 # in dB - parameter by default: 40\n",
    "\n",
    "    # Define dissonance model and parameters\n",
    "    fmin = 50 # in Hertz default: 50\n",
    "    fmax = sr/2 # in Hertz (max value possible)\n",
    "    dissonance_model_str = 'sethares1993'\n",
    "    n_consonant_partials = 5\n",
    "\n",
    "\n",
    "    y_harm = librosa.effects.harmonic(y=y, margin=8)    \n",
    "    \n",
    "    # Obtain center frequencies of spectrogram representation in Hertz\n",
    "    stft_frequencies = librosa.core.fft_frequencies(sr=sr,n_fft=n_fft) \n",
    "\n",
    "    # Compute short-term Fourier transform (STFT) modulus\n",
    "    stft = librosa.stft(y_harm, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "    abs2_stft = np.abs(stft)**2\n",
    "\n",
    "    # Truncate in frequency\n",
    "    stft_lowest_bin = np.where(stft_frequencies > fmin)[0][0]\n",
    "    stft_highest_bin = np.where(stft_frequencies < fmax)[0][-1]\n",
    "    S = abs2_stft[stft_lowest_bin:stft_highest_bin, :]\n",
    "    logS = librosa.amplitude_to_db(S)\n",
    "\n",
    "    # Run onset detection\n",
    "\n",
    "    onset_frames = librosa.onset.onset_detect(y_harm, sr=sr, hop_length=hop_length)\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # Loop over onsets\n",
    "    note_dissonances = []\n",
    "    for onset_id, onset_frame in enumerate(onset_frames):\n",
    "\n",
    "        # Extract energy peaks\n",
    "        logS_note = logS[:, onset_frame]\n",
    "        thresholded_logS_note = np.maximum(logS_note, loudness_threshold)\n",
    "        peak_locs, _ = scipy.signal.find_peaks(thresholded_logS_note)\n",
    "\n",
    "        # Convert peak locations to peak frequencies in Hertz\n",
    "        peak_freqs = np.array([stft_frequencies[peak_loc] for peak_loc in list(peak_locs)])\n",
    "\n",
    "        # If there are no peaks, append a NaN (\"not a number\") value\n",
    "        if len(peak_freqs) == 0:\n",
    "            max_pairwise_dissonance = np.nan\n",
    "        else:\n",
    "        # Otherwise, append maximum amount of dissonance\n",
    "        # between lowest frequency and all other frequencies\n",
    "            fundamental_freq = peak_freqs[0]\n",
    "            pairwise_dissonances = []\n",
    "            # Loop over upper frequencies\n",
    "            for freq in peak_freqs:\n",
    "                freq_pair = [fundamental_freq, freq]\n",
    "                h_freqs, h_amps = dissonant.harmonic_tone(freq_pair, n_partials=n_consonant_partials)\n",
    "                pairwise_dissonance = dissonant.dissonance(h_freqs, h_amps, model=dissonance_model_str)\n",
    "                pairwise_dissonances.append(pairwise_dissonance)\n",
    "            max_pairwise_dissonance = np.max(pairwise_dissonances)\n",
    "        note_dissonances.append(np.array(max_pairwise_dissonance))\n",
    "        \n",
    "    note_dissonances = np.apply_along_axis(pad, 0, note_dissonances)\n",
    "    \n",
    "    ##Plotting the dissonance curve\n",
    "    fg = plt.figure(figsize=(20, 4))\n",
    "    #plt.subplot(1, 1, 1)\n",
    "    plt.plot(onset_times, note_dissonances, 'c')\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Dissonance\")\n",
    "    plt.title(\"Dissonance\")\n",
    "    plt.ylim(0, max(note_dissonances)+ 0.2)\n",
    "    \n",
    "    return note_dissonances, onset_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ae154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_standardization(t_tempo, tempo, loudness, pitch_df, t_dissonance, dissonance, t_onset, onset_freq):\n",
    "    \n",
    "    #Tempo & Loudness\n",
    "    df_tup = list(zip(t_tempo, tempo, loudness[0]))\n",
    "    df_tempo_loudness= pd.DataFrame(df_tup, columns = [\"time\", \"tempo\", \"loudness\"])\n",
    "\n",
    "    df_tempo_loudness['tempo_z'] = stats.zscore(df_tempo_loudness['tempo'])\n",
    "    df_tempo_loudness['loudness_z'] = stats.zscore(df_tempo_loudness['loudness'])\n",
    "\n",
    "    \n",
    "    #Pitch \n",
    "\n",
    "    #Integrating melody and bass to use all pitches for standardization\n",
    "    df_long = pd.melt(pitch_df, id_vars='time')\n",
    "    df_long[\"pitch_z\"] = (df_long.value - df_long.value.mean())/df_long.value.std(ddof=0)\n",
    "\n",
    "    #Separating the pitches again \n",
    "    #So now, we have the two pitch lines but standardized on the basis of the two lines together\n",
    "    df_wide_pitch_z = df_long.pivot(index='time', columns='variable', values='pitch_z')\n",
    "    df_wide_pitch_z = df_wide_pitch_z.reset_index()\n",
    "\n",
    "    #Merging pitch with tempo and loudness\n",
    "    df_loudness_tempo_pitch = pd.merge(df_wide_pitch_z, df_tempo_loudness.iloc[:,[0,3,4]], on = \"time\", how = \"left\")\n",
    "\n",
    "    \n",
    "    #Dissonance\n",
    "    df_tup = zip(t_dissonance, dissonance)\n",
    "    df_dissonance= pd.DataFrame(df_tup, columns = [\"time\", \"dissonance\"])\n",
    "    df_dissonance = df_dissonance.dropna()\n",
    "    df_dissonance['dissonance_z'] = stats.zscore(df_dissonance['dissonance'])\n",
    "    \n",
    "    #Onset frequency\n",
    "    df_tup = list(zip(t_onset, onset_freq))\n",
    "    df_onset_freq= pd.DataFrame(df_tup, columns = [\"time\", \"onset_freq\"])\n",
    "\n",
    "    df_onset_freq['onset_freq_z'] = stats.zscore(df_onset_freq['onset_freq'])\n",
    "\n",
    "    ## Merging --\n",
    "    #Onset frequency\n",
    "    merge_onset = pd.merge(df_loudness_tempo_pitch,df_onset_freq.iloc[:,[0,2]],on='time', how='left')\n",
    "\n",
    "    merge_onset.loc[pd.notnull(merge_onset.onset_freq_z)]\n",
    "    \n",
    "    #Dissonance\n",
    "    df_all_features = pd.merge(merge_onset, df_dissonance.iloc[:,[0,2]], on = \"time\", how = 'left')\n",
    "\n",
    "    df_all_features.loc[pd.notnull(df_all_features.dissonance_z)]\n",
    "    ##Padding nas for plotting\n",
    "    df_plot = df_all_features.fillna(method='ffill')\n",
    "\n",
    "    return df_all_features, df_plot, pitch_df, onset_env"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
